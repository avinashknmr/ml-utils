<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ml_utils.woe_binning API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{font-family:Avenir,Calibri,Verdana;line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ml_utils.woe_binning</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pandas as pd
import numpy as np
import math
import warnings
import copy

def woe_binning_3 (df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good):
    cutpoints_backup = False
    stop_limit_exceeded = False
    list_level_a_collected = False
    iv_total_collect = pd.DataFrame([[np.nan]], columns=[&#39;iv_total_collect&#39;])
    
    #### Build subsets with target and predictor variable
    df = df[[target_var, pred_var]] # used for final binning
    dfrm = df[[target_var, pred_var]]# used for iterative merging of bins
    dfrm.columns = [&#39;target_var&#39;,&#39;predictor_var&#39;]

    #### Check if numerical variable or factor was provided as predictor and apply appropriate binning technique

    ### Binning in case a numerical variable was selected
    if len(dfrm.iloc[:,0].drop_duplicates()) == 2 and (dfrm.iloc[:,1].dtypes.kind in &#39;bifc&#39;) == True:
        

        ## Derive number of initial bins from min.perc.total parameter
        max_bins = math.trunc(1/min_perc_total)
        
        ## Derive cutpoints for bins (with similar frequency)
        cutpoints = dfrm.predictor_var.quantile(np.arange(0,max_bins+1)/max_bins).reset_index(drop=True)
        innercutpoints = [-np.inf] + list(cutpoints[1:len(cutpoints)-1]) + [np.inf]  # add -Inf, +Inf to cutpoints
        cutpoints = list(dict.fromkeys(innercutpoints))
        cutpoints = [int(i) if abs(i)!=np.inf else i for i in cutpoints]
        return cutpoints

def woe_binning_2 (df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good):
    cutpoints_backup = False
    stop_limit_exceeded = False
    list_level_a_collected = False
    iv_total_collect = pd.DataFrame([[np.nan]], columns=[&#39;iv_total_collect&#39;])
    
    #### Build subsets with target and predictor variable
    df = df[[target_var, pred_var]] # used for final binning
    dfrm = df[[target_var, pred_var]]# used for iterative merging of bins
    dfrm.columns = [&#39;target_var&#39;,&#39;predictor_var&#39;]

    #### Check if numerical variable or factor was provided as predictor and apply appropriate binning technique

    ### Binning in case a numerical variable was selected
    if len(dfrm.iloc[:,0].drop_duplicates()) == 2 and (dfrm.iloc[:,1].dtypes.kind in &#39;bifc&#39;) == True:
        

        ## Derive number of initial bins from min.perc.total parameter
        max_bins = math.trunc(1/min_perc_total)
        
        ## Derive cutpoints for bins (with similar frequency)
        cutpoints = dfrm.predictor_var.quantile(np.arange(0,max_bins+1)/max_bins).reset_index(drop=True)
        innercutpoints = [-np.inf] + list(cutpoints[1:len(cutpoints)-1]) + [np.inf]  # add -Inf, +Inf to cutpoints
        cutpoints = list(dict.fromkeys(innercutpoints))
        cutpoints = [int(i) if abs(i)!=np.inf else i for i in cutpoints]
        
        ## Calculate initial crosstab from binned variable and target variable
        ## to identify and merge sparse bins
        
        
        # Compute binned variable from cutpoints and add it to the subset data frame
        dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
            retbins=False, precision=10, include_lowest=False)
            
            
        # Compute crosstab from binned variable and target variable and covert it to a data frame
        freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
        freq_table = freq_table.reset_index(drop=False)
        missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                              0.0: [dfrm.isnull().sum(axis = 0)[0]],
                              1.0: [dfrm.isnull().sum(axis = 0)[1]]})
        freq_table =freq_table.append(missing,ignore_index=True, sort=False)
        woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
        woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
        
        # Compute columns percents for target classes from crosstab frequencies
        woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
        woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
        
        # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
        if df.iloc[:,1].isnull().values.any()==False:
            if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                        
        else:
            if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)

        # Check for bins (without last regular and without NA bin) if frequencies &lt; percentage limit specified above
    # (in reverse order to remain correct reference to cutpoints)
        for i in reversed(range(0,len(woe_dfrm.index)-2)):
            if woe_dfrm[&#34;col_perc_a&#34;].iloc[i]&lt;min_perc_class or woe_dfrm[&#34;col_perc_b&#34;].iloc[i]&lt;min_perc_class or (woe_dfrm.iloc[i][0] + woe_dfrm.iloc[i][1])/(sum(woe_dfrm[0]) + sum(woe_dfrm[1]))&lt;min_perc_total:
                # Remove cutpoint 
                del cutpoints[i+1]
                # Compute binned variable from cutpoints and add it to the subset data frame
                dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
                    retbins=False, precision=10, include_lowest=False)
                # Compute crosstab from binned variable and target variable and covert it to a data frame   
                freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
                freq_table = freq_table.reset_index(drop=False)
                missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                                      0.0: [dfrm.isnull().sum(axis = 0)[0]],
                                      1.0: [dfrm.isnull().sum(axis = 0)[1]]})
                freq_table =freq_table.append(missing,ignore_index=True, sort=False)
                woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
                woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
                # Compute columns percents for target classes from crosstab frequencies
                woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
                woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
                # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
                if df.iloc[:,1].isnull().values.any()==False:
                    if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                                
                else:
                    if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)
            # Stop in case 3 cutpoints (-Inf, x, +Inf) are reached
            if len(cutpoints)==3:
                break
          
            # Check for last regular bin if frequencies &lt; percentage limit specified above (only in case number of cutpoints &gt; 3
        if len(cutpoints)&gt;3:
            if woe_dfrm[&#34;col_perc_a&#34;][len(woe_dfrm.index)-2]&lt;min_perc_class or woe_dfrm[&#34;col_perc_b&#34;][len(woe_dfrm.index)-2]&lt;min_perc_class or (woe_dfrm.iloc[len(woe_dfrm.index)-2,0] + woe_dfrm.iloc[len(woe_dfrm.index)-2,1])/(sum(woe_dfrm[0])+sum(woe_dfrm[1]))&lt;min_perc_total:
                # Remove cutpoint
                del cutpoints[len(woe_dfrm.index)-2]
                # Compute binned variable from cutpoints and add it to the subset data frame
                dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
                    retbins=False, precision=10, include_lowest=False)
                # Compute crosstab from binned variable and target variable and covert it to a data frame 
                freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
                freq_table = freq_table.reset_index(drop=False)
                missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                                      0.0: [dfrm.isnull().sum(axis = 0)[0]],
                                      1.0: [dfrm.isnull().sum(axis = 0)[1]]})
                freq_table =freq_table.append(missing,ignore_index=True, sort=False)
                woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
                woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
                #woe_dfrm = woe_dfrm[[&#39;good&#39;, &#39;bad&#39;]]  # Select columns with raw frequencies only
                # Compute columns percents for target classes from crosstab frequencies
                woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
                woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
                # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
                if df.iloc[:,1].isnull().values.any()==False:
                    if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                                
                else:
                    if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)
                        
        ## After sparse bins are merged:
        ## Merge bins with similar WOE values and calculate corresponding WOE table and IV step by step
        ## until 2 bins are left (i.e. 3 cutpoints: -Inf, middle cutpoint, +Inf)
        while len(cutpoints)&gt;2:
    
            # Compute binned variable from cutpoints and add it to the subset data frame
            dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
                    retbins=False, precision=10, include_lowest=False)
            
            # Compute crosstab from binned variable and target variable and covert it to a data frame
            freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
            freq_table = freq_table.reset_index(drop=False)
            missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                                  0.0: [dfrm.isnull().sum(axis = 0)[0]],
                                  1.0: [dfrm.isnull().sum(axis = 0)[1]]})
            freq_table =freq_table.append(missing,ignore_index=True, sort=False)
            woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
            woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
            
            # Compute columns percents for target classes from crosstab frequencies
            woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
            woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
            # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
            if df.iloc[:,1].isnull().values.any()==False:
                if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                    woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                    woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                                
            else:
                if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                    woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                    woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)   

            woe_dfrm[&#34;woe&#34;] = 100*np.log(woe_dfrm[&#34;col_perc_a&#34;]/woe_dfrm[&#34;col_perc_b&#34;])
    
            woe_dfrm_list = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            woe_lag = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            
            woe_lag.insert(0, np.nan)  #add Nan [0]
            woe_lag.pop(len(woe_lag)-1) #delete Nan
            
            woe_dfrm_list=pd.DataFrame(woe_dfrm_list) #list to dataframe
            woe_lag= pd.DataFrame(woe_lag) #list to dataframe
            
            woe_diff = (woe_dfrm_list-woe_lag).abs()
            
            woe_dfrm[&#34;woe_lag&#34;] = list(woe_lag.iloc[:,0]) #add column woe_lag to woe_dfrm
            woe_dfrm[&#34;woe_diff&#34;] = list(woe_diff.iloc[:,0]) #add column woe_diff to woe_dfrm
            
            woe_dfrm[&#34;iv_bins&#34;] = (woe_dfrm[&#34;col_perc_a&#34;]-woe_dfrm[&#34;col_perc_b&#34;])*woe_dfrm[&#34;woe&#34;]/100
            
            # Calculate total IV for current binning
            iv_total = sum(woe_dfrm.fillna(0)[&#39;iv_bins&#39;])
            iv_total = pd.DataFrame([[iv_total]], columns=[&#39;iv_total&#39;])
            
            # Collect total IVs for different binning solutions
            if np.isnan(iv_total_collect.iloc[0][0])==False:
                iv_total_collect = pd.concat([iv_total_collect, iv_total], axis=1)
            else:
                iv_total_collect.iloc[0][0] = iv_total.iloc[0][0]
            
            # In case IV decreases by more than percentage specified by stop.limit parameter above
            # restore former binning solution (cutpoints) and leave loop
            if len(iv_total_collect.columns)&gt;1:
                actual_iv_decrease = ((iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])-(iv_total_collect.iloc[:,len(iv_total_collect.columns)-1]))/(iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])
        
                if actual_iv_decrease.iloc[0] &gt; stop_limit and stop_limit_exceeded == False:
                    cutpoints_final = cutpoints_backup
                    woe_dfrm_final = woe_dfrm_backup
                    stop_limit_exceeded = True   # indicates that stop limit is exceeded to prevent overriding the final solution
            
            # Save first cutpoint solution and corresponding WOE values as final solution (is used in case no WOE merging will be applied)
            if cutpoints_backup == False:
                cutpoints_final = cutpoints
                woe_dfrm_final = woe_dfrm

            # Saves binning solution after last merging step in case the IV stop limit was not exceeded
            if stop_limit_exceeded == False and len(cutpoints)==3:
                cutpoints_final = cutpoints
                woe_dfrm_final = woe_dfrm

            # Save backups of current cutpoints and corresponding WOE values before merging to be able to retrieve solution in case IV decrease is too strong
            cutpoints_backup = copy.deepcopy(cutpoints)
            woe_dfrm_backup = woe_dfrm
    
            # Determine the index of the minimum WOE difference between adjacent bins and
            # merge bins with minimum WOE difference (apart from the last &#39;Missing&#39; bin)    
            min_woe_diff = woe_dfrm[&#34;woe_diff&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1])==min(woe_dfrm[&#34;woe_diff&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]).fillna(+np.inf))
            min_woe_diff = min_woe_diff.reset_index(drop=True).index[min_woe_diff == True].tolist()
            min_woe_diff = min_woe_diff[0]
            del cutpoints[min_woe_diff] 
            
        ## Compute final IV
        iv_total_final = sum(woe_dfrm_final.fillna(0)[&#34;iv_bins&#34;])
        
        ## Save final binning solution via look-up-table for deployment
        lower_cutpoints_final_dfrm = pd.DataFrame(cutpoints_final, columns=[&#39;cutpoints_final&#39;])
        
        upper_cutpoints_final_dfrm = pd.DataFrame(cutpoints_final[1:]+[&#39;Missing&#39;], columns=[&#39;upper_cutpoints_final_dfrm&#39;])
        iv_total_final = pd.DataFrame([[iv_total_final]], columns=[&#39;iv_total_final&#39;])
        look_up_table = pd.concat([woe_dfrm_final.iloc[:,3].reset_index(drop=False), lower_cutpoints_final_dfrm, upper_cutpoints_final_dfrm, iv_total_final], axis=1).set_index([&#34;predictor_var_binned&#34;])
        look_up_table = pd.concat([look_up_table, woe_dfrm_final.iloc[:,0], woe_dfrm_final.iloc[:,1], woe_dfrm_final.iloc[:,2], woe_dfrm_final.iloc[:,6]], axis=1) # add column with final total Information Value
        look_up_table[&#34;iv_total_final&#34;] = look_up_table[&#34;iv_total_final&#34;].fillna(method = &#39;ffill&#39;)
        
        if bad == 0 and good == 1:
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;bad&#34;, 1: &#34;good&#34;})
        elif good == 0 and bad==1:    
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;good&#34;, 1: &#34;bad&#34;})  
        
        binning = pd.concat([(woe_dfrm_final[&#34;woe&#34;]).reset_index(drop=False), look_up_table[&#34;cutpoints_final&#34;].reset_index(drop=True), look_up_table[&#34;upper_cutpoints_final_dfrm&#34;].reset_index(drop=True), look_up_table[&#34;iv_total_final&#34;].reset_index(drop=True), look_up_table[&#34;good&#34;].reset_index(drop=True), look_up_table[&#34;bad&#34;].reset_index(drop=True), look_up_table[&#34;col_perc_a&#34;].reset_index(drop=True), look_up_table[&#34;col_perc_b&#34;].reset_index(drop=True), woe_dfrm_final[&#34;iv_bins&#34;].reset_index(drop=True)], axis=1, sort=False).set_index([&#34;predictor_var_binned&#34;])
    
    
    
    
    
    
    
    ### Binning in case a factor was selected        
    if len(dfrm.iloc[:,0].drop_duplicates()) == 2 and (dfrm.iloc[:,1].dtype == &#39;object&#39;)==True:
        dfrm.iloc[:,1] = dfrm.iloc[:,1].astype(&#39;category&#39;)            
        ## Copy predictor variable to prepare binning/recoding
        dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var&#34;]
        
        ## Handling of NAs
        if dfrm[&#34;predictor_var_binned&#34;].isnull().values.any()==True:
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([&#34;Missing&#34;])   # add factor level &#39;Missing&#39;
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(&#34;Missing&#34;)   # replace NA with string &#39;Missing&#39;
            
        ## Prepare binned factor in INPUT data (levels may be merged in subsequent steps)
        df[pred_var+&#34;_binned&#34;] = df[pred_var].astype(&#39;category&#39;)
        #df.loc[df[pred_var+&#34;_binned&#34;]] = df[pred_var].astype(&#39;category&#39;)
    
        # Handling of NAs
        if df.iloc[:,len(df.columns)-1].isnull().values.any()==True:
            df.iloc[:,len(df.columns)-1]=df.iloc[:,len(df.columns)-1].cat.add_categories([&#34;Missing&#34;]) # add factor level &#39;Missing&#39;
            df.iloc[:,len(df.columns)-1]=df.iloc[:,len(df.columns)-1].fillna(&#34;Missing&#34;)  # replace NA with string &#39;Missing&#39;
    
        
        ## Calculate initial crosstab from binned variable and target variable
        ## to identify and merge sparse bins
        
        # Compute crosstab from binned variable and target variable and covert it to a data frame
        freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;])
        woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
        # Compute WOE and information value (IV) from crosstab frequencies
        woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
        woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
        
        # Correct column percents in case of 0 frequencies
        if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
            woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
            woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)
    
        # Merge factor levels with frequencies &lt; percentage limit specified above to &#34;misc. level&#34; (associated with pos. and neg. WOE values)
        woe_dfrm[&#34;sparse_merge&#34;]=np.nan
        woe_dfrm.loc[(woe_dfrm[&#34;col_perc_a&#34;]&lt;min_perc_class) | (woe_dfrm[&#34;col_perc_b&#34;]&lt;min_perc_class) | (((woe_dfrm.iloc[:,0]+woe_dfrm.iloc[:,1])/((woe_dfrm.iloc[:,0].sum())+(woe_dfrm.iloc[:,1].sum())))&lt;min_perc_total), &#34;sparse_merge&#34;] = 1
        woe_dfrm_sparse_subset = woe_dfrm[woe_dfrm[&#34;sparse_merge&#34;]==1]
        
        woe_dfrm_sparse_subset.loc[(woe_dfrm_sparse_subset[&#34;col_perc_a&#34;]&lt;= woe_dfrm_sparse_subset[&#34;col_perc_b&#34;]),&#34;sparse_merge&#34;] =-1
               
        woe_dfrm_sparse_subset_pos = woe_dfrm_sparse_subset[woe_dfrm_sparse_subset[&#34;sparse_merge&#34;]==1]
        woe_dfrm_sparse_subset_neg = woe_dfrm_sparse_subset[woe_dfrm_sparse_subset[&#34;sparse_merge&#34;]==-1]
        
        if len(list(woe_dfrm_sparse_subset_pos.index))&gt;0:
            for i in range (0,len(list(woe_dfrm_sparse_subset_pos.index))):
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(list(woe_dfrm_sparse_subset_pos.index)[i])
                
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([&#34;misc. level pos.&#34;])   # add factor level &#39;Missing&#39;
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(&#34;misc. level pos.&#34;)   # replace NA with string &#39;Missing&#39;
          
        if len(list(woe_dfrm_sparse_subset_neg.index))&gt;0:    
            for i in range (0,len(list(woe_dfrm_sparse_subset_neg.index))):
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(list(woe_dfrm_sparse_subset_neg.index)[i]) 
            
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([&#34;misc. level neg.&#34;])   # add factor level &#39;Missing&#39;
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(&#34;misc. level neg.&#34;)   # replace NA with string &#39;Missing&#39;
        
        ## After sparse levels are merged:
        ## Merge levels with similar WOE values and calculate corresponding WOE table and IV step by step until
        ## 2 regular bins (+ Missing or &#39;misc. level&#39;) are left
    
        while len(dfrm[&#34;predictor_var_binned&#34;].cat.categories)&gt;3:
        
            # Compute crosstab from binned variable and target variable and covert it to a data frame
            freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;])
            woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
            # Compute WOE and information value (IV) from crosstab frequencies
            woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
            woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
        
            # Correct column percents in case of 0 frequencies
            if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)   
    
    
            woe_dfrm[&#34;woe&#34;] = 100*np.log(woe_dfrm[&#34;col_perc_a&#34;]/woe_dfrm[&#34;col_perc_b&#34;])
            woe_dfrm = woe_dfrm.sort_values(by=[&#39;woe&#39;])
        
            woe_dfrm_list = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            woe_lag = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            
            woe_lag.insert(0, np.nan)  #add Nan [0]
            woe_lag.pop(len(woe_lag)-1) #delete Nan
            
            woe_dfrm_list=pd.DataFrame(woe_dfrm_list) #list to dataframe
            woe_lag= pd.DataFrame(woe_lag) #list to dataframe
            
            woe_diff = (woe_dfrm_list-woe_lag).abs()
            
            woe_dfrm[&#34;woe_lag&#34;] = list(woe_lag.iloc[:,0]) #add column woe_lag to woe_dfrm
            woe_dfrm[&#34;woe_diff&#34;] = list(woe_diff.iloc[:,0]) #add column woe_diff to woe_dfrm
            
            woe_dfrm[&#34;iv_bins&#34;] = (woe_dfrm[&#34;col_perc_a&#34;]-woe_dfrm[&#34;col_perc_b&#34;])*woe_dfrm[&#34;woe&#34;]/100
            
            # Calculate total IV for current binning
            iv_total = sum(woe_dfrm.fillna(0)[&#39;iv_bins&#39;])
            iv_total = pd.DataFrame([[iv_total]], columns=[&#39;iv_total&#39;])
    
            # Collect total IVs for different binning solutions
            if np.isnan(iv_total_collect.iloc[0][0])==False:
                iv_total_collect = pd.concat([iv_total_collect, iv_total], axis=1)
            else:
                iv_total_collect.iloc[0][0] = iv_total.iloc[0][0]
            
            # In case IV decreases by more than percentage specified by stop.limit parameter above
            # restore former binning solution (cutpoints) and leave loop
            if len(iv_total_collect.columns)&gt;1:
                actual_iv_decrease = ((iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])-(iv_total_collect.iloc[:,len(iv_total_collect.columns)-1]))/(iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])
        
                if actual_iv_decrease.iloc[0] &gt; stop_limit and stop_limit_exceeded == False:
                    stop_limit_exceeded = True   # indicates that stop limit is exceeded to prevent overriding the final solution
            
            # Merge until 2 regular bins remain  
            if len(dfrm[&#34;predictor_var_binned&#34;].cat.categories)&gt;3:
                
                # Merge levels with most similar WOE values
                min_woe_diff = woe_dfrm[&#34;woe_diff&#34;]==min(woe_dfrm[&#34;woe_diff&#34;].fillna(+np.inf))
                min_woe_diff_index = min_woe_diff.reset_index(drop=True).index[min_woe_diff == True].tolist()[0]
                
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(min_woe_diff.index[min_woe_diff_index])
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(min_woe_diff.index[min_woe_diff_index-1]) 
                dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([min_woe_diff.index[min_woe_diff_index]+ &#34; + &#34; +min_woe_diff.index[min_woe_diff_index-1]])
                dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(min_woe_diff.index[min_woe_diff_index]+ &#34; + &#34;+min_woe_diff.index[min_woe_diff_index-1])
                
                
                # Save names of the factor levels that are merged
                list_level_a = [woe_dfrm.index[min_woe_diff_index]]
                list_level_b = [woe_dfrm.index[min_woe_diff_index-1]]
                
                # Collect names of the factor levels that are merged in lists (until stop criteria is reached)
                if list_level_a_collected == False:
                    list_level_a_collected = list_level_a
                    list_level_b_collected = list_level_b
                else:
                    if stop_limit_exceeded == False:
                        list_level_a_collected = list_level_a_collected + list_level_a
                        list_level_b_collected = list_level_b_collected + list_level_b
    
                    else:
                        list_level_a_collected = list_level_a_collected[0:len(list_level_a_collected)]
                        list_level_b_collected = list_level_b_collected[0:len(list_level_b_collected)]
                                    
        ### Apply FINAL binning to INPUT data
        
        ## Merge factor levels
        # Merge sparse levels
        df.iloc[:,len(df.columns)-1] = df.iloc[:,len(df.columns)-1].cat.add_categories([&#34;misc. level pos.&#34;,&#34;misc. level neg.&#34;])
        for i in range (0,len(list(woe_dfrm_sparse_subset_pos.index))):
            df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list(woe_dfrm_sparse_subset_pos.index)[i])
        df.iloc[:,len(df.columns)-1] = df.iloc[:,len(df.columns)-1].fillna(&#34;misc. level pos.&#34;)
        for i in range (0,len(list(woe_dfrm_sparse_subset_neg.index))):
            df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list(woe_dfrm_sparse_subset_neg.index)[i])
        df.iloc[:,len(df.columns)-1] = df.iloc[:,len(df.columns)-1].fillna(&#34;misc. level neg.&#34;)
        
        # Merge levels with similar WOE values
        if list_level_a_collected != False:
            for i in range(0,len(list_level_a_collected)): 
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list_level_a_collected[i])
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list_level_b_collected[i])
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.add_categories([list_level_a_collected[i]+ &#34; + &#34; + list_level_b_collected[i]])
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].fillna(list_level_a_collected[i]+ &#34; + &#34; + list_level_b_collected[i])
    
        ## Repeat generating WOE table for selected binning solution
        
        # Compute crosstab from binned variable and target variable and covert it to a data frame
        freq_table_final = pd.crosstab(df.iloc[:,len(df.columns)-1],dfrm[&#34;target_var&#34;])
        woe_dfrm_final = pd.DataFrame(freq_table_final) # Convert frequency table to data frame
    
        # Compute WOE and information value (IV) from crosstab frequencies
        woe_dfrm_final[&#34;col_perc_a&#34;] = woe_dfrm_final[good]/sum(woe_dfrm_final[good])
        woe_dfrm_final[&#34;col_perc_b&#34;] = woe_dfrm_final[bad]/sum(woe_dfrm_final[bad])
        # Correct column percents in case of 0 frequencies
        if min(woe_dfrm_final.iloc[:,0])==0 or min(woe_dfrm_final.iloc[:,1])==0:
                woe_dfrm_final[&#34;col_perc_a&#34;] = (woe_dfrm_final[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm_final[&#34;col_perc_a&#34;] + 0.0001)
                woe_dfrm_final[&#34;col_perc_b&#34;] = (woe_dfrm_final[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm_final[&#34;col_perc_b&#34;] + 0.0001)  
         
        woe_dfrm_final[&#34;woe&#34;] = 100*np.log(woe_dfrm_final[&#34;col_perc_a&#34;]/woe_dfrm_final[&#34;col_perc_b&#34;])
        woe_dfrm_final = woe_dfrm_final.sort_values(by=[&#39;woe&#39;])
        woe_dfrm_final[&#34;iv_bins&#34;] = (woe_dfrm_final[&#34;col_perc_a&#34;]-woe_dfrm_final[&#34;col_perc_b&#34;])*woe_dfrm_final[&#34;woe&#34;]/100
        iv_total_final = sum(woe_dfrm_final.fillna(0)[&#39;iv_bins&#39;])
        iv_total_final = pd.DataFrame([[iv_total_final]], columns=[&#39;iv_total_final&#39;])
        
           
        ## Add variable with corresponding WOE values for final binning
        
        # Add final binned (numerical) variable with WOE values (via left join with WOE table)
    
        df = pd.merge(df,woe_dfrm_final[&#34;woe&#34;].to_frame(), how=&#39;left&#39;, left_on=df.columns[len(df.columns)-1], right_index=True)
        df= df.rename(index=str, columns={df.columns[len(df.columns)-1]: pred_var+&#34;_binned_woe&#34;})
    
        ## Save final binning solution via look-up-table for deployment
        df[pred_var] = df[pred_var].astype(&#39;category&#39;)
        df[pred_var]=df[pred_var].cat.add_categories([&#34;Missing&#34;]) # add factor level &#39;Missing&#39;
        df[pred_var] = df[pred_var].fillna(&#34;Missing&#34;)   # replace NA with string &#39;Missing&#39;
        
        look_up_table = df.groupby([df[pred_var],df[df.columns[len(df.columns)-2]]])
        look_up_table = look_up_table[[df.columns[len(df.columns)-1]]].mean().dropna().reset_index(drop=False)
        
        look_up_table =pd.concat([look_up_table.iloc[:,1],look_up_table.drop(columns=[look_up_table.columns[1]])], axis=1)
        look_up_table = look_up_table.rename(index=str, columns={look_up_table.columns[0]: &#34;Group_2&#34;, look_up_table.columns[1]: &#34;Group_1&#34;})
        
        look_up_table = pd.concat([look_up_table.reset_index(drop=True), iv_total_final], axis=1)   # add column with final total Information Value
        look_up_table[&#34;iv_total_final&#34;] = look_up_table[&#34;iv_total_final&#34;].fillna(method = &#39;ffill&#39;)
        look_up_table = look_up_table.rename(index=str, columns={look_up_table.columns[2]: &#34;woe&#34;})
        
        
        look_up_table = pd.merge(look_up_table, woe_dfrm_final.drop(columns=[&#39;woe&#39;]), how=&#39;left&#39;, left_on=look_up_table.columns[0], right_index=True)
        look_up_table = look_up_table.sort_values(by=[&#39;woe&#39;,look_up_table.columns[0]]) # sort by woe value and merged bin name
        

        # In case the misc. level consists only of only NA rename it &#39;Missing&#39;
        if len(look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;])==1 and len(look_up_table[look_up_table.iloc[:,0]==&#34;misc. level neg.&#34;])==1:
            if look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;].index[0] == look_up_table[(look_up_table.iloc[:,0]==&#34;misc. level neg.&#34;) &amp; (look_up_table.iloc[:,1]==&#34;Missing&#34;)].index[0]:
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.add_categories([&#34;Missing&#34;])   # add factor level &#39;Missing&#39;
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.remove_categories(&#34;misc. level neg.&#34;)
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].fillna(&#34;Missing&#34;)   # replace NA with string &#34;Missing&#34;
            
        if len(look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;])==1 and len(look_up_table[look_up_table.iloc[:,0]==&#34;misc. level pos.&#34;])==1:
            if look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;].index[0] == look_up_table[(look_up_table.iloc[:,0]==&#34;misc. level pos.&#34;) &amp; (look_up_table.iloc[:,1]==&#34;Missing&#34;)].index[0]:   
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.add_categories([&#34;Missing&#34;])   # add factor level &#39;Missing&#39;
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.remove_categories(&#34;misc. level pos.&#34;)
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].fillna(&#34;Missing&#34;)   # replace NA with string &#34;Missing&#34;

        # Abbreviate long factor levels (in case they are longer than specified or longer than 1000 characters)
        if abbrev_fact_levels==0 and 1000&lt;look_up_table.iloc[:,1].str.len().max():
            abbrev_fact_levels = 1000
    
            
        if bad == 0 and good == 1:
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;bad&#34;, 1: &#34;good&#34;})
        elif good == 0 and bad== 1:    
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;good&#34;, 1: &#34;bad&#34;})  
            
        binning=look_up_table
            
             
    #### Check for correct variable specification and
    #### generate requested output, in case specification is correct
    
    ### Display warning message in case of incorrect predictor variable specification
    
    if (dfrm.iloc[:,1].dtypes.kind in &#39;bifc&#39;) == False and (dfrm.iloc[:,1].dtypes==&#34;category&#34;)==False:
        warnings.warn(&#34;Incorrect variable specification.\nPredictor variable needs to be a numeric variable or a factor.&#34;)

    
    ### Generate requested output, in case specification is correct
    
    else:
        ## Function passes the final binning solution as look-up table
        look_up_table
        
    return binning

def woe_binning (df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, event_class):
#### Warning message and defaults in case parameters are not specified
    if df.isnull().values.any()==True or type(target_var) is str == False or type(pred_var) is str == False:
        warnings.warn(&#34;Incorrect specification of data frame and/or variables.&#34;)
    
    if pd.isnull(min_perc_total)==True:
        min_perc_total=0.05
        
    if min_perc_total&lt;0.0001 or min_perc_total&gt;0.2 or (str(min_perc_total).replace(&#39;.&#39;,&#39;&#39;,1).isdigit()) == False:
        warnings.warn(&#34;Incorrect parameter specification; accepted min.perc.total parameter range is 0.0001-0.2. Parameter was set to default (0.05).&#34;)
        min_perc_total=0.05

    if pd.isnull(min_perc_class)==True:
        min_perc_class=0
        
    if min_perc_class&lt;0 or min_perc_class&gt;0.2 or (str(min_perc_class).replace(&#39;.&#39;,&#39;&#39;,1).isdigit()) == False:
        warnings.warn(&#34;Incorrect parameter specification; accepted min.perc.class parameter range is 0-0.2. Parameter was set to default (0).&#34;)
        min_perc_class=0
        
    if pd.isnull(stop_limit)==True:
        stop_limit=0.1
        
    if stop_limit&lt;0 or stop_limit&gt;0.5 or (str(stop_limit).replace(&#39;.&#39;,&#39;&#39;,1).isdigit()) == False:
        warnings.warn(&#34;Incorrect parameter specification; accepted stop.limit parameter range is 0-0.05. Parameter was set to default (0.1).&#34;)
        stop_limit=0.1
        
    if pd.isnull(abbrev_fact_levels)==True:
        abbrev_fact_levels=200
        
    if abbrev_fact_levels&lt;0 or abbrev_fact_levels&gt;1000:
        warnings.warn(&#34;Incorrect parameter specification; accepted abbrev.fact.levels parameter range is 0-10000. Parameter was set to default (200).&#34;)
        abbrev_fact_levels=200

    #### Display warning message in case of incorrect target variable specification
    if len(df[target_var].drop_duplicates().isna())!=2:
        warnings.warn(&#34;Incorrect variable specification.\nTarget variable must have two distinct values (NAs are accepted).&#34;)

    #### Display warning message in case none of the target classes matches the specified event.class parameter
    if pd.isnull(event_class)==False:
        if df[target_var].drop_duplicates().iloc[0]==event_class or df[target_var].drop_duplicates().iloc[1]==event_class==False:
            warnings.warn(&#34;None of the target classes matches the specified event.class parameter.&#34;)
    
    #### In case bad class was specified assign &#39;good&#39; and &#39;bad&#39; codes (the latter will be associated with negative WOE values then)
    if pd.isnull(event_class)==False: 
        if df[target_var].drop_duplicates().iloc[0]==event_class:
            bad = df[target_var].drop_duplicates().iloc[0]
            good = df[target_var].drop_duplicates().iloc[1]
        else:
            bad = df[target_var].drop_duplicates().iloc[1]
            good = df[target_var].drop_duplicates().iloc[0]
            
    else:
        bad = df[target_var].drop_duplicates().iloc[0]
        good = df[target_var].drop_duplicates().iloc[1]
        
    bad = int(bad)
    good = int(good)

    #### Subset: consider only cases without NA in target variable
    df = df[df[target_var].isna()==False]
    
    #### Call actual binning function and put binning solutions together with respective variable names into a list
    woe_binning_2(df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good)

    return woe_binning_2(df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ml_utils.woe_binning.woe_binning"><code class="name flex">
<span>def <span class="ident">woe_binning</span></span>(<span>df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, event_class)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def woe_binning (df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, event_class):
#### Warning message and defaults in case parameters are not specified
    if df.isnull().values.any()==True or type(target_var) is str == False or type(pred_var) is str == False:
        warnings.warn(&#34;Incorrect specification of data frame and/or variables.&#34;)
    
    if pd.isnull(min_perc_total)==True:
        min_perc_total=0.05
        
    if min_perc_total&lt;0.0001 or min_perc_total&gt;0.2 or (str(min_perc_total).replace(&#39;.&#39;,&#39;&#39;,1).isdigit()) == False:
        warnings.warn(&#34;Incorrect parameter specification; accepted min.perc.total parameter range is 0.0001-0.2. Parameter was set to default (0.05).&#34;)
        min_perc_total=0.05

    if pd.isnull(min_perc_class)==True:
        min_perc_class=0
        
    if min_perc_class&lt;0 or min_perc_class&gt;0.2 or (str(min_perc_class).replace(&#39;.&#39;,&#39;&#39;,1).isdigit()) == False:
        warnings.warn(&#34;Incorrect parameter specification; accepted min.perc.class parameter range is 0-0.2. Parameter was set to default (0).&#34;)
        min_perc_class=0
        
    if pd.isnull(stop_limit)==True:
        stop_limit=0.1
        
    if stop_limit&lt;0 or stop_limit&gt;0.5 or (str(stop_limit).replace(&#39;.&#39;,&#39;&#39;,1).isdigit()) == False:
        warnings.warn(&#34;Incorrect parameter specification; accepted stop.limit parameter range is 0-0.05. Parameter was set to default (0.1).&#34;)
        stop_limit=0.1
        
    if pd.isnull(abbrev_fact_levels)==True:
        abbrev_fact_levels=200
        
    if abbrev_fact_levels&lt;0 or abbrev_fact_levels&gt;1000:
        warnings.warn(&#34;Incorrect parameter specification; accepted abbrev.fact.levels parameter range is 0-10000. Parameter was set to default (200).&#34;)
        abbrev_fact_levels=200

    #### Display warning message in case of incorrect target variable specification
    if len(df[target_var].drop_duplicates().isna())!=2:
        warnings.warn(&#34;Incorrect variable specification.\nTarget variable must have two distinct values (NAs are accepted).&#34;)

    #### Display warning message in case none of the target classes matches the specified event.class parameter
    if pd.isnull(event_class)==False:
        if df[target_var].drop_duplicates().iloc[0]==event_class or df[target_var].drop_duplicates().iloc[1]==event_class==False:
            warnings.warn(&#34;None of the target classes matches the specified event.class parameter.&#34;)
    
    #### In case bad class was specified assign &#39;good&#39; and &#39;bad&#39; codes (the latter will be associated with negative WOE values then)
    if pd.isnull(event_class)==False: 
        if df[target_var].drop_duplicates().iloc[0]==event_class:
            bad = df[target_var].drop_duplicates().iloc[0]
            good = df[target_var].drop_duplicates().iloc[1]
        else:
            bad = df[target_var].drop_duplicates().iloc[1]
            good = df[target_var].drop_duplicates().iloc[0]
            
    else:
        bad = df[target_var].drop_duplicates().iloc[0]
        good = df[target_var].drop_duplicates().iloc[1]
        
    bad = int(bad)
    good = int(good)

    #### Subset: consider only cases without NA in target variable
    df = df[df[target_var].isna()==False]
    
    #### Call actual binning function and put binning solutions together with respective variable names into a list
    woe_binning_2(df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good)

    return woe_binning_2(df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good)</code></pre>
</details>
</dd>
<dt id="ml_utils.woe_binning.woe_binning_2"><code class="name flex">
<span>def <span class="ident">woe_binning_2</span></span>(<span>df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def woe_binning_2 (df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good):
    cutpoints_backup = False
    stop_limit_exceeded = False
    list_level_a_collected = False
    iv_total_collect = pd.DataFrame([[np.nan]], columns=[&#39;iv_total_collect&#39;])
    
    #### Build subsets with target and predictor variable
    df = df[[target_var, pred_var]] # used for final binning
    dfrm = df[[target_var, pred_var]]# used for iterative merging of bins
    dfrm.columns = [&#39;target_var&#39;,&#39;predictor_var&#39;]

    #### Check if numerical variable or factor was provided as predictor and apply appropriate binning technique

    ### Binning in case a numerical variable was selected
    if len(dfrm.iloc[:,0].drop_duplicates()) == 2 and (dfrm.iloc[:,1].dtypes.kind in &#39;bifc&#39;) == True:
        

        ## Derive number of initial bins from min.perc.total parameter
        max_bins = math.trunc(1/min_perc_total)
        
        ## Derive cutpoints for bins (with similar frequency)
        cutpoints = dfrm.predictor_var.quantile(np.arange(0,max_bins+1)/max_bins).reset_index(drop=True)
        innercutpoints = [-np.inf] + list(cutpoints[1:len(cutpoints)-1]) + [np.inf]  # add -Inf, +Inf to cutpoints
        cutpoints = list(dict.fromkeys(innercutpoints))
        cutpoints = [int(i) if abs(i)!=np.inf else i for i in cutpoints]
        
        ## Calculate initial crosstab from binned variable and target variable
        ## to identify and merge sparse bins
        
        
        # Compute binned variable from cutpoints and add it to the subset data frame
        dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
            retbins=False, precision=10, include_lowest=False)
            
            
        # Compute crosstab from binned variable and target variable and covert it to a data frame
        freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
        freq_table = freq_table.reset_index(drop=False)
        missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                              0.0: [dfrm.isnull().sum(axis = 0)[0]],
                              1.0: [dfrm.isnull().sum(axis = 0)[1]]})
        freq_table =freq_table.append(missing,ignore_index=True, sort=False)
        woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
        woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
        
        # Compute columns percents for target classes from crosstab frequencies
        woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
        woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
        
        # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
        if df.iloc[:,1].isnull().values.any()==False:
            if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                        
        else:
            if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)

        # Check for bins (without last regular and without NA bin) if frequencies &lt; percentage limit specified above
    # (in reverse order to remain correct reference to cutpoints)
        for i in reversed(range(0,len(woe_dfrm.index)-2)):
            if woe_dfrm[&#34;col_perc_a&#34;].iloc[i]&lt;min_perc_class or woe_dfrm[&#34;col_perc_b&#34;].iloc[i]&lt;min_perc_class or (woe_dfrm.iloc[i][0] + woe_dfrm.iloc[i][1])/(sum(woe_dfrm[0]) + sum(woe_dfrm[1]))&lt;min_perc_total:
                # Remove cutpoint 
                del cutpoints[i+1]
                # Compute binned variable from cutpoints and add it to the subset data frame
                dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
                    retbins=False, precision=10, include_lowest=False)
                # Compute crosstab from binned variable and target variable and covert it to a data frame   
                freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
                freq_table = freq_table.reset_index(drop=False)
                missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                                      0.0: [dfrm.isnull().sum(axis = 0)[0]],
                                      1.0: [dfrm.isnull().sum(axis = 0)[1]]})
                freq_table =freq_table.append(missing,ignore_index=True, sort=False)
                woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
                woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
                # Compute columns percents for target classes from crosstab frequencies
                woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
                woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
                # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
                if df.iloc[:,1].isnull().values.any()==False:
                    if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                                
                else:
                    if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)
            # Stop in case 3 cutpoints (-Inf, x, +Inf) are reached
            if len(cutpoints)==3:
                break
          
            # Check for last regular bin if frequencies &lt; percentage limit specified above (only in case number of cutpoints &gt; 3
        if len(cutpoints)&gt;3:
            if woe_dfrm[&#34;col_perc_a&#34;][len(woe_dfrm.index)-2]&lt;min_perc_class or woe_dfrm[&#34;col_perc_b&#34;][len(woe_dfrm.index)-2]&lt;min_perc_class or (woe_dfrm.iloc[len(woe_dfrm.index)-2,0] + woe_dfrm.iloc[len(woe_dfrm.index)-2,1])/(sum(woe_dfrm[0])+sum(woe_dfrm[1]))&lt;min_perc_total:
                # Remove cutpoint
                del cutpoints[len(woe_dfrm.index)-2]
                # Compute binned variable from cutpoints and add it to the subset data frame
                dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
                    retbins=False, precision=10, include_lowest=False)
                # Compute crosstab from binned variable and target variable and covert it to a data frame 
                freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
                freq_table = freq_table.reset_index(drop=False)
                missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                                      0.0: [dfrm.isnull().sum(axis = 0)[0]],
                                      1.0: [dfrm.isnull().sum(axis = 0)[1]]})
                freq_table =freq_table.append(missing,ignore_index=True, sort=False)
                woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
                woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
                #woe_dfrm = woe_dfrm[[&#39;good&#39;, &#39;bad&#39;]]  # Select columns with raw frequencies only
                # Compute columns percents for target classes from crosstab frequencies
                woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
                woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
                # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
                if df.iloc[:,1].isnull().values.any()==False:
                    if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                                
                else:
                    if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                        woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                        woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)
                        
        ## After sparse bins are merged:
        ## Merge bins with similar WOE values and calculate corresponding WOE table and IV step by step
        ## until 2 bins are left (i.e. 3 cutpoints: -Inf, middle cutpoint, +Inf)
        while len(cutpoints)&gt;2:
    
            # Compute binned variable from cutpoints and add it to the subset data frame
            dfrm[&#34;predictor_var_binned&#34;] = pd.cut(dfrm[&#34;predictor_var&#34;], cutpoints, right=True, labels = None,
                    retbins=False, precision=10, include_lowest=False)
            
            # Compute crosstab from binned variable and target variable and covert it to a data frame
            freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;], dropna=True)
            freq_table = freq_table.reset_index(drop=False)
            missing=pd.DataFrame({&#39;predictor_var_binned&#39;: [&#34;Missing&#34;],
                                  0.0: [dfrm.isnull().sum(axis = 0)[0]],
                                  1.0: [dfrm.isnull().sum(axis = 0)[1]]})
            freq_table =freq_table.append(missing,ignore_index=True, sort=False)
            woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
            woe_dfrm = woe_dfrm.set_index([&#34;predictor_var_binned&#34;])
            
            # Compute columns percents for target classes from crosstab frequencies
            woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
            woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
            # Correct column percents in case of 0 frequencies (in case of no NA skip last row)
            if df.iloc[:,1].isnull().values.any()==False:
                if min(woe_dfrm.iloc[:,0].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0 or min(woe_dfrm.iloc[:,1].drop(woe_dfrm.index[len(woe_dfrm)-1]))==0:
                    woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                    woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]) + 0.0001)
                                
            else:
                if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                    woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                    woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)   

            woe_dfrm[&#34;woe&#34;] = 100*np.log(woe_dfrm[&#34;col_perc_a&#34;]/woe_dfrm[&#34;col_perc_b&#34;])
    
            woe_dfrm_list = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            woe_lag = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            
            woe_lag.insert(0, np.nan)  #add Nan [0]
            woe_lag.pop(len(woe_lag)-1) #delete Nan
            
            woe_dfrm_list=pd.DataFrame(woe_dfrm_list) #list to dataframe
            woe_lag= pd.DataFrame(woe_lag) #list to dataframe
            
            woe_diff = (woe_dfrm_list-woe_lag).abs()
            
            woe_dfrm[&#34;woe_lag&#34;] = list(woe_lag.iloc[:,0]) #add column woe_lag to woe_dfrm
            woe_dfrm[&#34;woe_diff&#34;] = list(woe_diff.iloc[:,0]) #add column woe_diff to woe_dfrm
            
            woe_dfrm[&#34;iv_bins&#34;] = (woe_dfrm[&#34;col_perc_a&#34;]-woe_dfrm[&#34;col_perc_b&#34;])*woe_dfrm[&#34;woe&#34;]/100
            
            # Calculate total IV for current binning
            iv_total = sum(woe_dfrm.fillna(0)[&#39;iv_bins&#39;])
            iv_total = pd.DataFrame([[iv_total]], columns=[&#39;iv_total&#39;])
            
            # Collect total IVs for different binning solutions
            if np.isnan(iv_total_collect.iloc[0][0])==False:
                iv_total_collect = pd.concat([iv_total_collect, iv_total], axis=1)
            else:
                iv_total_collect.iloc[0][0] = iv_total.iloc[0][0]
            
            # In case IV decreases by more than percentage specified by stop.limit parameter above
            # restore former binning solution (cutpoints) and leave loop
            if len(iv_total_collect.columns)&gt;1:
                actual_iv_decrease = ((iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])-(iv_total_collect.iloc[:,len(iv_total_collect.columns)-1]))/(iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])
        
                if actual_iv_decrease.iloc[0] &gt; stop_limit and stop_limit_exceeded == False:
                    cutpoints_final = cutpoints_backup
                    woe_dfrm_final = woe_dfrm_backup
                    stop_limit_exceeded = True   # indicates that stop limit is exceeded to prevent overriding the final solution
            
            # Save first cutpoint solution and corresponding WOE values as final solution (is used in case no WOE merging will be applied)
            if cutpoints_backup == False:
                cutpoints_final = cutpoints
                woe_dfrm_final = woe_dfrm

            # Saves binning solution after last merging step in case the IV stop limit was not exceeded
            if stop_limit_exceeded == False and len(cutpoints)==3:
                cutpoints_final = cutpoints
                woe_dfrm_final = woe_dfrm

            # Save backups of current cutpoints and corresponding WOE values before merging to be able to retrieve solution in case IV decrease is too strong
            cutpoints_backup = copy.deepcopy(cutpoints)
            woe_dfrm_backup = woe_dfrm
    
            # Determine the index of the minimum WOE difference between adjacent bins and
            # merge bins with minimum WOE difference (apart from the last &#39;Missing&#39; bin)    
            min_woe_diff = woe_dfrm[&#34;woe_diff&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1])==min(woe_dfrm[&#34;woe_diff&#34;].drop(woe_dfrm.index[len(woe_dfrm)-1]).fillna(+np.inf))
            min_woe_diff = min_woe_diff.reset_index(drop=True).index[min_woe_diff == True].tolist()
            min_woe_diff = min_woe_diff[0]
            del cutpoints[min_woe_diff] 
            
        ## Compute final IV
        iv_total_final = sum(woe_dfrm_final.fillna(0)[&#34;iv_bins&#34;])
        
        ## Save final binning solution via look-up-table for deployment
        lower_cutpoints_final_dfrm = pd.DataFrame(cutpoints_final, columns=[&#39;cutpoints_final&#39;])
        
        upper_cutpoints_final_dfrm = pd.DataFrame(cutpoints_final[1:]+[&#39;Missing&#39;], columns=[&#39;upper_cutpoints_final_dfrm&#39;])
        iv_total_final = pd.DataFrame([[iv_total_final]], columns=[&#39;iv_total_final&#39;])
        look_up_table = pd.concat([woe_dfrm_final.iloc[:,3].reset_index(drop=False), lower_cutpoints_final_dfrm, upper_cutpoints_final_dfrm, iv_total_final], axis=1).set_index([&#34;predictor_var_binned&#34;])
        look_up_table = pd.concat([look_up_table, woe_dfrm_final.iloc[:,0], woe_dfrm_final.iloc[:,1], woe_dfrm_final.iloc[:,2], woe_dfrm_final.iloc[:,6]], axis=1) # add column with final total Information Value
        look_up_table[&#34;iv_total_final&#34;] = look_up_table[&#34;iv_total_final&#34;].fillna(method = &#39;ffill&#39;)
        
        if bad == 0 and good == 1:
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;bad&#34;, 1: &#34;good&#34;})
        elif good == 0 and bad==1:    
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;good&#34;, 1: &#34;bad&#34;})  
        
        binning = pd.concat([(woe_dfrm_final[&#34;woe&#34;]).reset_index(drop=False), look_up_table[&#34;cutpoints_final&#34;].reset_index(drop=True), look_up_table[&#34;upper_cutpoints_final_dfrm&#34;].reset_index(drop=True), look_up_table[&#34;iv_total_final&#34;].reset_index(drop=True), look_up_table[&#34;good&#34;].reset_index(drop=True), look_up_table[&#34;bad&#34;].reset_index(drop=True), look_up_table[&#34;col_perc_a&#34;].reset_index(drop=True), look_up_table[&#34;col_perc_b&#34;].reset_index(drop=True), woe_dfrm_final[&#34;iv_bins&#34;].reset_index(drop=True)], axis=1, sort=False).set_index([&#34;predictor_var_binned&#34;])
    
    
    
    
    
    
    
    ### Binning in case a factor was selected        
    if len(dfrm.iloc[:,0].drop_duplicates()) == 2 and (dfrm.iloc[:,1].dtype == &#39;object&#39;)==True:
        dfrm.iloc[:,1] = dfrm.iloc[:,1].astype(&#39;category&#39;)            
        ## Copy predictor variable to prepare binning/recoding
        dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var&#34;]
        
        ## Handling of NAs
        if dfrm[&#34;predictor_var_binned&#34;].isnull().values.any()==True:
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([&#34;Missing&#34;])   # add factor level &#39;Missing&#39;
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(&#34;Missing&#34;)   # replace NA with string &#39;Missing&#39;
            
        ## Prepare binned factor in INPUT data (levels may be merged in subsequent steps)
        df[pred_var+&#34;_binned&#34;] = df[pred_var].astype(&#39;category&#39;)
        #df.loc[df[pred_var+&#34;_binned&#34;]] = df[pred_var].astype(&#39;category&#39;)
    
        # Handling of NAs
        if df.iloc[:,len(df.columns)-1].isnull().values.any()==True:
            df.iloc[:,len(df.columns)-1]=df.iloc[:,len(df.columns)-1].cat.add_categories([&#34;Missing&#34;]) # add factor level &#39;Missing&#39;
            df.iloc[:,len(df.columns)-1]=df.iloc[:,len(df.columns)-1].fillna(&#34;Missing&#34;)  # replace NA with string &#39;Missing&#39;
    
        
        ## Calculate initial crosstab from binned variable and target variable
        ## to identify and merge sparse bins
        
        # Compute crosstab from binned variable and target variable and covert it to a data frame
        freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;])
        woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
        # Compute WOE and information value (IV) from crosstab frequencies
        woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
        woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
        
        # Correct column percents in case of 0 frequencies
        if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
            woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
            woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)
    
        # Merge factor levels with frequencies &lt; percentage limit specified above to &#34;misc. level&#34; (associated with pos. and neg. WOE values)
        woe_dfrm[&#34;sparse_merge&#34;]=np.nan
        woe_dfrm.loc[(woe_dfrm[&#34;col_perc_a&#34;]&lt;min_perc_class) | (woe_dfrm[&#34;col_perc_b&#34;]&lt;min_perc_class) | (((woe_dfrm.iloc[:,0]+woe_dfrm.iloc[:,1])/((woe_dfrm.iloc[:,0].sum())+(woe_dfrm.iloc[:,1].sum())))&lt;min_perc_total), &#34;sparse_merge&#34;] = 1
        woe_dfrm_sparse_subset = woe_dfrm[woe_dfrm[&#34;sparse_merge&#34;]==1]
        
        woe_dfrm_sparse_subset.loc[(woe_dfrm_sparse_subset[&#34;col_perc_a&#34;]&lt;= woe_dfrm_sparse_subset[&#34;col_perc_b&#34;]),&#34;sparse_merge&#34;] =-1
               
        woe_dfrm_sparse_subset_pos = woe_dfrm_sparse_subset[woe_dfrm_sparse_subset[&#34;sparse_merge&#34;]==1]
        woe_dfrm_sparse_subset_neg = woe_dfrm_sparse_subset[woe_dfrm_sparse_subset[&#34;sparse_merge&#34;]==-1]
        
        if len(list(woe_dfrm_sparse_subset_pos.index))&gt;0:
            for i in range (0,len(list(woe_dfrm_sparse_subset_pos.index))):
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(list(woe_dfrm_sparse_subset_pos.index)[i])
                
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([&#34;misc. level pos.&#34;])   # add factor level &#39;Missing&#39;
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(&#34;misc. level pos.&#34;)   # replace NA with string &#39;Missing&#39;
          
        if len(list(woe_dfrm_sparse_subset_neg.index))&gt;0:    
            for i in range (0,len(list(woe_dfrm_sparse_subset_neg.index))):
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(list(woe_dfrm_sparse_subset_neg.index)[i]) 
            
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([&#34;misc. level neg.&#34;])   # add factor level &#39;Missing&#39;
            dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(&#34;misc. level neg.&#34;)   # replace NA with string &#39;Missing&#39;
        
        ## After sparse levels are merged:
        ## Merge levels with similar WOE values and calculate corresponding WOE table and IV step by step until
        ## 2 regular bins (+ Missing or &#39;misc. level&#39;) are left
    
        while len(dfrm[&#34;predictor_var_binned&#34;].cat.categories)&gt;3:
        
            # Compute crosstab from binned variable and target variable and covert it to a data frame
            freq_table = pd.crosstab(dfrm[&#34;predictor_var_binned&#34;],dfrm[&#34;target_var&#34;])
            woe_dfrm = pd.DataFrame(freq_table) # Convert frequency table to data frame
            # Compute WOE and information value (IV) from crosstab frequencies
            woe_dfrm[&#34;col_perc_a&#34;] = woe_dfrm[good]/sum(woe_dfrm[good])
            woe_dfrm[&#34;col_perc_b&#34;] = woe_dfrm[bad]/sum(woe_dfrm[bad])
        
            # Correct column percents in case of 0 frequencies
            if min(woe_dfrm.iloc[:,0])==0 or min(woe_dfrm.iloc[:,1])==0:
                woe_dfrm[&#34;col_perc_a&#34;] = (woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_a&#34;] + 0.0001)
                woe_dfrm[&#34;col_perc_b&#34;] = (woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm[&#34;col_perc_b&#34;] + 0.0001)   
    
    
            woe_dfrm[&#34;woe&#34;] = 100*np.log(woe_dfrm[&#34;col_perc_a&#34;]/woe_dfrm[&#34;col_perc_b&#34;])
            woe_dfrm = woe_dfrm.sort_values(by=[&#39;woe&#39;])
        
            woe_dfrm_list = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            woe_lag = list(woe_dfrm[&#34;woe&#34;]) #dataframe to list
            
            woe_lag.insert(0, np.nan)  #add Nan [0]
            woe_lag.pop(len(woe_lag)-1) #delete Nan
            
            woe_dfrm_list=pd.DataFrame(woe_dfrm_list) #list to dataframe
            woe_lag= pd.DataFrame(woe_lag) #list to dataframe
            
            woe_diff = (woe_dfrm_list-woe_lag).abs()
            
            woe_dfrm[&#34;woe_lag&#34;] = list(woe_lag.iloc[:,0]) #add column woe_lag to woe_dfrm
            woe_dfrm[&#34;woe_diff&#34;] = list(woe_diff.iloc[:,0]) #add column woe_diff to woe_dfrm
            
            woe_dfrm[&#34;iv_bins&#34;] = (woe_dfrm[&#34;col_perc_a&#34;]-woe_dfrm[&#34;col_perc_b&#34;])*woe_dfrm[&#34;woe&#34;]/100
            
            # Calculate total IV for current binning
            iv_total = sum(woe_dfrm.fillna(0)[&#39;iv_bins&#39;])
            iv_total = pd.DataFrame([[iv_total]], columns=[&#39;iv_total&#39;])
    
            # Collect total IVs for different binning solutions
            if np.isnan(iv_total_collect.iloc[0][0])==False:
                iv_total_collect = pd.concat([iv_total_collect, iv_total], axis=1)
            else:
                iv_total_collect.iloc[0][0] = iv_total.iloc[0][0]
            
            # In case IV decreases by more than percentage specified by stop.limit parameter above
            # restore former binning solution (cutpoints) and leave loop
            if len(iv_total_collect.columns)&gt;1:
                actual_iv_decrease = ((iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])-(iv_total_collect.iloc[:,len(iv_total_collect.columns)-1]))/(iv_total_collect.iloc[:,len(iv_total_collect.columns)-2])
        
                if actual_iv_decrease.iloc[0] &gt; stop_limit and stop_limit_exceeded == False:
                    stop_limit_exceeded = True   # indicates that stop limit is exceeded to prevent overriding the final solution
            
            # Merge until 2 regular bins remain  
            if len(dfrm[&#34;predictor_var_binned&#34;].cat.categories)&gt;3:
                
                # Merge levels with most similar WOE values
                min_woe_diff = woe_dfrm[&#34;woe_diff&#34;]==min(woe_dfrm[&#34;woe_diff&#34;].fillna(+np.inf))
                min_woe_diff_index = min_woe_diff.reset_index(drop=True).index[min_woe_diff == True].tolist()[0]
                
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(min_woe_diff.index[min_woe_diff_index])
                dfrm[&#34;predictor_var_binned&#34;]= dfrm[&#34;predictor_var_binned&#34;].cat.remove_categories(min_woe_diff.index[min_woe_diff_index-1]) 
                dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].cat.add_categories([min_woe_diff.index[min_woe_diff_index]+ &#34; + &#34; +min_woe_diff.index[min_woe_diff_index-1]])
                dfrm[&#34;predictor_var_binned&#34;] = dfrm[&#34;predictor_var_binned&#34;].fillna(min_woe_diff.index[min_woe_diff_index]+ &#34; + &#34;+min_woe_diff.index[min_woe_diff_index-1])
                
                
                # Save names of the factor levels that are merged
                list_level_a = [woe_dfrm.index[min_woe_diff_index]]
                list_level_b = [woe_dfrm.index[min_woe_diff_index-1]]
                
                # Collect names of the factor levels that are merged in lists (until stop criteria is reached)
                if list_level_a_collected == False:
                    list_level_a_collected = list_level_a
                    list_level_b_collected = list_level_b
                else:
                    if stop_limit_exceeded == False:
                        list_level_a_collected = list_level_a_collected + list_level_a
                        list_level_b_collected = list_level_b_collected + list_level_b
    
                    else:
                        list_level_a_collected = list_level_a_collected[0:len(list_level_a_collected)]
                        list_level_b_collected = list_level_b_collected[0:len(list_level_b_collected)]
                                    
        ### Apply FINAL binning to INPUT data
        
        ## Merge factor levels
        # Merge sparse levels
        df.iloc[:,len(df.columns)-1] = df.iloc[:,len(df.columns)-1].cat.add_categories([&#34;misc. level pos.&#34;,&#34;misc. level neg.&#34;])
        for i in range (0,len(list(woe_dfrm_sparse_subset_pos.index))):
            df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list(woe_dfrm_sparse_subset_pos.index)[i])
        df.iloc[:,len(df.columns)-1] = df.iloc[:,len(df.columns)-1].fillna(&#34;misc. level pos.&#34;)
        for i in range (0,len(list(woe_dfrm_sparse_subset_neg.index))):
            df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list(woe_dfrm_sparse_subset_neg.index)[i])
        df.iloc[:,len(df.columns)-1] = df.iloc[:,len(df.columns)-1].fillna(&#34;misc. level neg.&#34;)
        
        # Merge levels with similar WOE values
        if list_level_a_collected != False:
            for i in range(0,len(list_level_a_collected)): 
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list_level_a_collected[i])
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.remove_categories(list_level_b_collected[i])
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].cat.add_categories([list_level_a_collected[i]+ &#34; + &#34; + list_level_b_collected[i]])
                df.iloc[:,len(df.columns)-1]= df.iloc[:,len(df.columns)-1].fillna(list_level_a_collected[i]+ &#34; + &#34; + list_level_b_collected[i])
    
        ## Repeat generating WOE table for selected binning solution
        
        # Compute crosstab from binned variable and target variable and covert it to a data frame
        freq_table_final = pd.crosstab(df.iloc[:,len(df.columns)-1],dfrm[&#34;target_var&#34;])
        woe_dfrm_final = pd.DataFrame(freq_table_final) # Convert frequency table to data frame
    
        # Compute WOE and information value (IV) from crosstab frequencies
        woe_dfrm_final[&#34;col_perc_a&#34;] = woe_dfrm_final[good]/sum(woe_dfrm_final[good])
        woe_dfrm_final[&#34;col_perc_b&#34;] = woe_dfrm_final[bad]/sum(woe_dfrm_final[bad])
        # Correct column percents in case of 0 frequencies
        if min(woe_dfrm_final.iloc[:,0])==0 or min(woe_dfrm_final.iloc[:,1])==0:
                woe_dfrm_final[&#34;col_perc_a&#34;] = (woe_dfrm_final[&#34;col_perc_a&#34;] + 0.0001)/sum(woe_dfrm_final[&#34;col_perc_a&#34;] + 0.0001)
                woe_dfrm_final[&#34;col_perc_b&#34;] = (woe_dfrm_final[&#34;col_perc_b&#34;] + 0.0001)/sum(woe_dfrm_final[&#34;col_perc_b&#34;] + 0.0001)  
         
        woe_dfrm_final[&#34;woe&#34;] = 100*np.log(woe_dfrm_final[&#34;col_perc_a&#34;]/woe_dfrm_final[&#34;col_perc_b&#34;])
        woe_dfrm_final = woe_dfrm_final.sort_values(by=[&#39;woe&#39;])
        woe_dfrm_final[&#34;iv_bins&#34;] = (woe_dfrm_final[&#34;col_perc_a&#34;]-woe_dfrm_final[&#34;col_perc_b&#34;])*woe_dfrm_final[&#34;woe&#34;]/100
        iv_total_final = sum(woe_dfrm_final.fillna(0)[&#39;iv_bins&#39;])
        iv_total_final = pd.DataFrame([[iv_total_final]], columns=[&#39;iv_total_final&#39;])
        
           
        ## Add variable with corresponding WOE values for final binning
        
        # Add final binned (numerical) variable with WOE values (via left join with WOE table)
    
        df = pd.merge(df,woe_dfrm_final[&#34;woe&#34;].to_frame(), how=&#39;left&#39;, left_on=df.columns[len(df.columns)-1], right_index=True)
        df= df.rename(index=str, columns={df.columns[len(df.columns)-1]: pred_var+&#34;_binned_woe&#34;})
    
        ## Save final binning solution via look-up-table for deployment
        df[pred_var] = df[pred_var].astype(&#39;category&#39;)
        df[pred_var]=df[pred_var].cat.add_categories([&#34;Missing&#34;]) # add factor level &#39;Missing&#39;
        df[pred_var] = df[pred_var].fillna(&#34;Missing&#34;)   # replace NA with string &#39;Missing&#39;
        
        look_up_table = df.groupby([df[pred_var],df[df.columns[len(df.columns)-2]]])
        look_up_table = look_up_table[[df.columns[len(df.columns)-1]]].mean().dropna().reset_index(drop=False)
        
        look_up_table =pd.concat([look_up_table.iloc[:,1],look_up_table.drop(columns=[look_up_table.columns[1]])], axis=1)
        look_up_table = look_up_table.rename(index=str, columns={look_up_table.columns[0]: &#34;Group_2&#34;, look_up_table.columns[1]: &#34;Group_1&#34;})
        
        look_up_table = pd.concat([look_up_table.reset_index(drop=True), iv_total_final], axis=1)   # add column with final total Information Value
        look_up_table[&#34;iv_total_final&#34;] = look_up_table[&#34;iv_total_final&#34;].fillna(method = &#39;ffill&#39;)
        look_up_table = look_up_table.rename(index=str, columns={look_up_table.columns[2]: &#34;woe&#34;})
        
        
        look_up_table = pd.merge(look_up_table, woe_dfrm_final.drop(columns=[&#39;woe&#39;]), how=&#39;left&#39;, left_on=look_up_table.columns[0], right_index=True)
        look_up_table = look_up_table.sort_values(by=[&#39;woe&#39;,look_up_table.columns[0]]) # sort by woe value and merged bin name
        

        # In case the misc. level consists only of only NA rename it &#39;Missing&#39;
        if len(look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;])==1 and len(look_up_table[look_up_table.iloc[:,0]==&#34;misc. level neg.&#34;])==1:
            if look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;].index[0] == look_up_table[(look_up_table.iloc[:,0]==&#34;misc. level neg.&#34;) &amp; (look_up_table.iloc[:,1]==&#34;Missing&#34;)].index[0]:
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.add_categories([&#34;Missing&#34;])   # add factor level &#39;Missing&#39;
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.remove_categories(&#34;misc. level neg.&#34;)
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].fillna(&#34;Missing&#34;)   # replace NA with string &#34;Missing&#34;
            
        if len(look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;])==1 and len(look_up_table[look_up_table.iloc[:,0]==&#34;misc. level pos.&#34;])==1:
            if look_up_table[look_up_table.iloc[:,1]==&#34;Missing&#34;].index[0] == look_up_table[(look_up_table.iloc[:,0]==&#34;misc. level pos.&#34;) &amp; (look_up_table.iloc[:,1]==&#34;Missing&#34;)].index[0]:   
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.add_categories([&#34;Missing&#34;])   # add factor level &#39;Missing&#39;
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].cat.remove_categories(&#34;misc. level pos.&#34;)
                look_up_table.iloc[:,0] = look_up_table.iloc[:,0].fillna(&#34;Missing&#34;)   # replace NA with string &#34;Missing&#34;

        # Abbreviate long factor levels (in case they are longer than specified or longer than 1000 characters)
        if abbrev_fact_levels==0 and 1000&lt;look_up_table.iloc[:,1].str.len().max():
            abbrev_fact_levels = 1000
    
            
        if bad == 0 and good == 1:
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;bad&#34;, 1: &#34;good&#34;})
        elif good == 0 and bad== 1:    
            look_up_table = look_up_table.rename(index=str, columns={0: &#34;good&#34;, 1: &#34;bad&#34;})  
            
        binning=look_up_table
            
             
    #### Check for correct variable specification and
    #### generate requested output, in case specification is correct
    
    ### Display warning message in case of incorrect predictor variable specification
    
    if (dfrm.iloc[:,1].dtypes.kind in &#39;bifc&#39;) == False and (dfrm.iloc[:,1].dtypes==&#34;category&#34;)==False:
        warnings.warn(&#34;Incorrect variable specification.\nPredictor variable needs to be a numeric variable or a factor.&#34;)

    
    ### Generate requested output, in case specification is correct
    
    else:
        ## Function passes the final binning solution as look-up table
        look_up_table
        
    return binning</code></pre>
</details>
</dd>
<dt id="ml_utils.woe_binning.woe_binning_3"><code class="name flex">
<span>def <span class="ident">woe_binning_3</span></span>(<span>df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def woe_binning_3 (df, target_var, pred_var, min_perc_total, min_perc_class, stop_limit, abbrev_fact_levels, bad, good):
    cutpoints_backup = False
    stop_limit_exceeded = False
    list_level_a_collected = False
    iv_total_collect = pd.DataFrame([[np.nan]], columns=[&#39;iv_total_collect&#39;])
    
    #### Build subsets with target and predictor variable
    df = df[[target_var, pred_var]] # used for final binning
    dfrm = df[[target_var, pred_var]]# used for iterative merging of bins
    dfrm.columns = [&#39;target_var&#39;,&#39;predictor_var&#39;]

    #### Check if numerical variable or factor was provided as predictor and apply appropriate binning technique

    ### Binning in case a numerical variable was selected
    if len(dfrm.iloc[:,0].drop_duplicates()) == 2 and (dfrm.iloc[:,1].dtypes.kind in &#39;bifc&#39;) == True:
        

        ## Derive number of initial bins from min.perc.total parameter
        max_bins = math.trunc(1/min_perc_total)
        
        ## Derive cutpoints for bins (with similar frequency)
        cutpoints = dfrm.predictor_var.quantile(np.arange(0,max_bins+1)/max_bins).reset_index(drop=True)
        innercutpoints = [-np.inf] + list(cutpoints[1:len(cutpoints)-1]) + [np.inf]  # add -Inf, +Inf to cutpoints
        cutpoints = list(dict.fromkeys(innercutpoints))
        cutpoints = [int(i) if abs(i)!=np.inf else i for i in cutpoints]
        return cutpoints</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ml_utils" href="index.html">ml_utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ml_utils.woe_binning.woe_binning" href="#ml_utils.woe_binning.woe_binning">woe_binning</a></code></li>
<li><code><a title="ml_utils.woe_binning.woe_binning_2" href="#ml_utils.woe_binning.woe_binning_2">woe_binning_2</a></code></li>
<li><code><a title="ml_utils.woe_binning.woe_binning_3" href="#ml_utils.woe_binning.woe_binning_3">woe_binning_3</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>